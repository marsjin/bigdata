1、集群节点配置

10.10.10.10  master.hadoop.mydomain.com
10.10.10.11  slave1.hadoop.mydomain.com
10.10.10.12  slave2.hadoop.mydomain.com

master1:NameNode,SecondaryNameNode,ResourceManage,NMaster
slave1:DataNode,NodeManager,HReginonserver
slave2:DataNode,NodeManager,HReginonserver

二 版本,路径和环境变量的设置

/usr/local/maven/maven-3.3.9
/usr/local/ant/apache-ant-1.9.7
/usr/local/java/jdk1.7.0_80
/usr/local/mysql(5.6 or later)


/etc/profile
export MAVEN_HOME=/usr/local/maven/maven-3.3.9
export ANT_HOME=/usr/local/ant/apache-ant-1.9.7
export JAVA_HOME=/usr/local/java/jdk1.7.0_80
export PATH=$PATH:$JAVA_HOME/bin:$ANT_HOME/bin:/usr/local/mysql/bin:$MAVEN_HOME/bin
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/jre/lib:$JAVA_HOME/lib/toos.jar:$ANT_HOME/lib/ant-launcher.jar:$ANT_HOME/lib/*.jar

安装路径
/letv/hadoop/hadoop-2.7.2
/letv/hive-2.1.0
/letv/hbase-1.2.2
/letv/zookeeper-3.4.8
/letv/sqoop-1.4.6
/letv/pig-0.16.0
/letv/sqoop-1.4.6


~/.bashrc或者~/.bash_profile
export HADOOP_HOME=/letv/hadoop/hadoop-2.7.2
export HIVE_HOME=/letv/hive-2.1.0
export SQOOP_HOME=/letv/sqoop-1.4.6
export HBASE_HOME=/letv/hbase-1.2.2
export ZOOKEEPER_HOME=/letv/zookeeper-3.4.8
export PIG_HOME=/letv/pig-0.16.0
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin:$ZOOKEEPER_HOME/bin:$HBASE_HOME/bin:$SQOOP_HOME/bin:$PIG_HOME/bin
export CLASSPATH=$CLASSPATH:$PIG_HOME/pig-0.16.0-core-h2.jar


基本工作
集群节点hostname为hadoop,hadoop1,hadoop2,虚拟机桥接设置
设置静态ip    vi /etc/sysconfig/network-scripts/ifcfg-eth0
更改hostname:
Centos
sudo echo hadoop1> /etc/sysconfig/network
ubuntu
sudo echo hadoop> /etc/hostname

ip绑定host
vim /etc/hosts
10.10.10.10  master.hadoop.mydomain.com
10.10.10.11  slave1.hadoop.mydomain.com
10.10.10.12  slave2.hadoop.mydomain.com



haodoop配置


/letv/hadoop/hadoop-2.7.2/etc/hadoop/hadoop-env.sh
export HADOOP_PREFIX=/letv/hadoop/hadoop-2.7.2
export JAVA_HOME=/usr/local/java/jdk1.7.0_80


/letv/hadoop/hadoop-2.7.2/etc/hadoop/core-site.xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://master.hadoop.mydomain.com:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/letv/hadoop/hadoop-2.7.2/data</value>
    </property>
    <property>
        <name>io.file.buffer.size</name>
        <value>131072</value>
    </property>
    <property>
        <name>hadoop.proxyuser.hadoop.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.hadoop.groups</name>
        <value>*</value>
    </property>
</configuration>


/letv/hadoop/hadoop-2.7.2/etc/hadoop/hdfs-site.xml
<configuration>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>master.hadoop.mydomain.com:50090</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.blocksize</name>
        <value>33554432</value>
    </property>

    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/letv/hadoop/data/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/letv/hadoop/data/dfs/data</value>
    </property>
    <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
    </property>
</configuration>


/letv/hadoop/hadoop-2.7.2/etc/hadoop/slaves
slave1.hadoop.mydomain.com
slave2.hadoop.mydomain.com


/letv/hadoop/hadoop-2.7.2/etc/hadoop/yarn-site.xml
<configuration>

    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <property>
        <name>yarn.resourcemanager.address</name>
        <value>master.hadoop.mydomain.com:8032</value>
        </property>
    <property>
        <name>yarn.resourcemanager.scheduler.address</name>
        <value>master.hadoop.mydomain.com:8030</value>
    </property>
    <property>
        <name>yarn.resourcemanager.resource-tracker.address</name>  
        <value>master.hadoop.mydomain.com:8031</value>
    </property>
    <property>
        <name>yarn.resourcemanager.admin.address</name>
        <value>master.hadoop.mydomain.com:8033</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>master.hadoop.mydomain.com:8088</value>
    </property>

    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master.hadoop.mydomain.com</value>
    </property>
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>604800</value>
    </property>
</configuration>


/letv/hadoop/hadoop-2.7.2/etc/hadoop/mapred-site.xml
<configuration>

    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.job.tracker</name>
        <value>hdfs://master.hadoop.mydomain.com:9001</value>
        <final>true</final>
    </property>
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>master.hadoop.mydomain.com:10020</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>master.hadoop.mydomain.com:19888</value>
    </property>
</configuration>

创建路径

/letv/hadoop/data/dfs/name
/letv/hadoop/data/dfs/data
/letv/hadoop/data/dfs/namesecondary


hive
/letv/hive-2.1.0/conf/hive-env.sh
export JAVA_HOME=/letv/java/jdk1.7.0_79
export HADOOP_HOME=/letv/hadoop/hadoop-2.7.2
export HIVE_CONF_DIR=/letv/hive-2.1.0/conf


/letv/hive-2.1.0/conf/hive-site.xml
<configuration>
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://10.10.10.10:3306/hive</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>123456</value>
    </property>

    <property>
        <name>hive.hwi.listen.host</name>
        <value>0.0.0.0</value>
    </property>
    <property>
        <name>hive.hwi.listen.port</name>
        <value>9999</value>
    </property>
    <property>
        <name>hive.hwi.war.file</name>
        <value>lib/hive-hwi-2.1.0.jar</value>
    </property>

    <property>
        <name>hive.querylog.location</name>
        <value>/letv/logs/hive</value>
    </property>
    <property>    
        <name>hive.aux.jars.path</name>     
        <value>file:///letv/hive-2.1.0/lib/hive-hbase-handler-2.1.0.jar,file:///letv/hive-2.1.0/lib/guava-14.0.1.jar,file:///letv/hive-2.1.0/lib/hbase-common-1.1.1.jar,file:///letv/hive-2.1.0/lib/zookeeper-3.4.6.jar</value>    
    </property>    
    <property>
        <name>hbase.zookeeper.quorum</name>
        <value>master.hadoop.mydomain.com:2181,slave1.hadoop.mydomain.com:2181,slave2.hadoop.mydomain.com:2181</value>
    </property>
</configuration>



zookeeper

/letv/zookeeper-3.4.8/conf/zoo.cfg
tickTime=2000
initLimit=10
syncLimit=5
dataDir=/letv/zookeeper-3.4.8/data
clientPort=2181
maxClientCnxns=60
server.1=master.hadoop.mydomain.com:2888:3888
server.2=slave1.hadoop.mydomain.com:2888:3888
server.3=slave2.hadoop.mydomain.com:2888:3888

/letv/zookeeper-3.4.8/data/myid
1


hbase
/letv/hbase-1.2.2/conf/hbase-env.sh
export HBASE_MANAGES_ZK=false
export JAVA_HOME=/letv/java/jdk1.7.0_79
export HBASE_CLASSPATH=/letv/hadoop/hadoop-2.7.2/etc/hadoop


/letv/hbase-1.2.2/conf/hbase-site.sh
<configuration>
  <property>
    <name>hbase.rootdir</name>
    <value>hdfs://master.hadoop.mydomain.com:9000/user/hbase</value>
  </property>
  <property>
    <name>hbase.cluster.distributed</name>
    <value>true</value>
  </property>
  <property>
    <name>hbase.zookeeper.property.clientPort</name>
    <value>2181</value>
  </property>
  <property>
    <name>hbase.zookeeper.quorum</name>
    <value>master.hadoop.mydomain.com,slave1.hadoop.mydomain.com,slave2.hadoop.mydomain.com</value>
  </property>
  <property>
    <name>hbase.zookeeper.property.dataDir</name>
    <value>/letv/hbase-1.2.2/data</value>
  </property>
  <property>
    <name>hbase.zookeeper.session.timeout</name>
    <value>90000</value>
  </property>
  <property>
    <name>hbase.tmp.dir</name>
    <value>/letv/hbase-1.2.2/data/tmp</value>
  </property>
</configuration


/letv/hbase-1.2.2/conf/regionservers
slave1.hadoop.mydomain.com
slave2.hadoop.mydomain.com


sqoop
/letv/sqoop-1.4.6/conf/sqoop-env.sh
export HADOOP_COMMON_HOME=/letv/hadoop/hadoop-2.7.2/
export HADOOP_MAPRED_HOME=/letv/hadoop/hadoop-2.7.2/
export HBASE_HOME=/letv/hbase-1.2.2
export HIVE_HOME=/letv/hive-2.1.0
export ZOOCFGDIR=/letv/zookeeper-3.4.8/conf

/letv/sqoop-1.4.6/bin/configure-sqoop注释一下内容
## Moved to be a runtime check in sqoop.
# if [ ! -d "${HCAT_HOME}" ]; then
#   echo "Warning: $HCAT_HOME does not exist! HCatalog jobs will fail."
#   echo 'Please set $HCAT_HOME to the root of your HCatalog installation.'
# fi

# if [ ! -d "${ACCUMULO_HOME}" ]; then
#   echo "Warning: $ACCUMULO_HOME does not exist! Accumulo imports will fail."
#　  echo 'Please set $ACCUMULO_HOME to the root of your Accumulo installation.'
#　fi

# Add HCatalog to dependency list
# if [ -e "${HCAT_HOME}/bin/hcat" ]; then
#   TMP_SQOOP_CLASSPATH=${SQOOP_CLASSPATH}:`${HCAT_HOME}/bin/hcat -classpath`
#   if [ -z "${HIVE_CONF_DIR}" ]; then
#     TMP_SQOOP_CLASSPATH=${TMP_SQOOP_CLASSPATH}:${HIVE_CONF_DIR}
#   fi
#   SQOOP_CLASSPATH=${TMP_SQOOP_CLASSPATH}
# fi

# Add Accumulo to dependency list
# if [ -e "$ACCUMULO_HOME/bin/accumulo" ]; then
#   for jn in `$ACCUMULO_HOME/bin/accumulo classpath | grep file:.*accumulo.*jar | cut -d':' -f2`; do
#     SQOOP_CLASSPATH=$SQOOP_CLASSPATH:$jn
#   done
#   for jn in `$ACCUMULO_HOME/bin/accumulo classpath | grep file:.*zookeeper.*jar | cut -d':' -f2`; do
#     SQOOP_CLASSPATH=$SQOOP_CLASSPATH:$jn
#   done
# fi


chown -r hadoop:hadoop hadoop hbase-1.2.2 hive-2.1.0 pig-0.16.0 sqoop-1.4.6 zookeeper-3.4.8

1．启动Hadoop的HDFS模块里的守护进程

HDFS里面的守护进程启动也有顺序，即：

1）启动NameNode守护进程；

2）启动DataNode守护进程；

3）启动SecondaryNameNode守护进程。

2．启动MapReduce模块里面的守护进程

MapReduce的守护进程启动也是有顺序的，即：

1）启动 JobTracker守护进程；

2）启动TaskTracker守护进程。

关闭的步骤正好相反


启动顺序
ZooKeeper
Hadoop
HBase
第二个HMaster

停止顺序

第二个 HMaster，kill-9 删除
Hbase
Hadoop
ZooKeeper
